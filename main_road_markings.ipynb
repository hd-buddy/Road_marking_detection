{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T15:48:25.437699Z","iopub.status.busy":"2024-04-06T15:48:25.436740Z","iopub.status.idle":"2024-04-06T15:48:25.445892Z","shell.execute_reply":"2024-04-06T15:48:25.444884Z","shell.execute_reply.started":"2024-04-06T15:48:25.437657Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","import os\n","import random\n","import cv2\n","import tensorflow as tf\n","from tensorflow.python.keras.models import Sequential\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input, Dropout\n","from tensorflow.keras.models import Model\n","from PIL import Image\n","import io\n","import seaborn as sns\n","sns.set_style('dark')\n","plt.rcParams['figure.figsize'] = (20,8)\n","plt.rcParams['font.size'] = 16"]},{"cell_type":"markdown","metadata":{},"source":["## Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T15:48:25.448262Z","iopub.status.busy":"2024-04-06T15:48:25.447975Z","iopub.status.idle":"2024-04-06T15:48:25.473418Z","shell.execute_reply":"2024-04-06T15:48:25.472586Z","shell.execute_reply.started":"2024-04-06T15:48:25.448240Z"},"trusted":true},"outputs":[],"source":["img_dim = 256\n","num_classes = 5\n","\n","class NPYDataGenerator(tf.keras.utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, images_path, masks_path, batch_size=32, dim=(256, 256), n_channels=3, n_classes=3, shuffle=True):\n","        'Initialization'\n","        self.dim = dim\n","        self.batch_size = batch_size\n","        self.images_path = images_path\n","        self.masks_path = masks_path\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.shuffle = shuffle\n","        self.image_ids = [os.path.splitext(file)[0] for file in os.listdir(images_path)]\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.image_ids) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        image_ids_temp = [self.image_ids[k] for k in indexes]\n","\n","        # Generate data\n","        X, y = self.__data_generation(image_ids_temp)\n","\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.image_ids))\n","        if self.shuffle:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, image_ids_temp):\n","        'Generates data containing batch_size samples' \n","        # Initialization\n","        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n","        y = np.empty((self.batch_size, *self.dim, self.n_classes))\n","\n","        # Generate data\n","        for i, ID in enumerate(image_ids_temp):\n","            # Store sample\n","            image_path = os.path.join(self.images_path, ID + '.jpg')\n","            image = cv2.imread(image_path)\n","            image = cv2.resize(image, self.dim)\n","            image = image / 255.0  # Normalize to [0, 1]\n","            X[i,] = image\n","\n","            # Store class\n","            mask_path = os.path.join(self.masks_path, ID + '.npy')\n","            mask = np.load(mask_path)\n","            y[i,] = mask\n","\n","        return X, y\n","\n","\n","# Set up the paths for the training and validation images and masks\n","train_images_path = '/kaggle/input/cv-road-marking/augmented_dataset/images/train' # Replace with your path to training images\n","train_masks_path = '/kaggle/input/cv-road-marking/augmented_dataset/masks/train' # Replace with your path to training masks\n","\n","val_images_path = '/kaggle/input/cv-road-marking/augmented_dataset/images/valid' # Replace with your path to validation images\n","val_masks_path = '/kaggle/input/cv-road-marking/augmented_dataset/masks/valid' # Replace with your path to validation masks\n","\n","test_images_path = '/kaggle/input/cv-road-marking/augmented_dataset/images/test' # Replace with your path to training images\n","test_masks_path = '/kaggle/input/cv-road-marking/augmented_dataset/masks/test'\n","# Create the data generators\n","train_generator = NPYDataGenerator(train_images_path, train_masks_path, batch_size=32, dim=(img_dim, img_dim), n_channels=3, n_classes=num_classes, shuffle=True)\n","val_generator = NPYDataGenerator(val_images_path, val_masks_path, batch_size=32, dim=(img_dim, img_dim), n_channels=3, n_classes=num_classes, shuffle=False)\n","test_generator = NPYDataGenerator(test_images_path, test_masks_path, batch_size=32, dim=(img_dim, img_dim), n_channels=3, n_classes=num_classes, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["## One hot Encoding of the Images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def one_hot_encode(mask_directory,output_directory):\n","    os.makedirs(output_directory, exist_ok=True)\n","    unique_colors = [[  0,   0,   0], # black\n","        [  0, 128, 128],\n","        [ 38,  75, 110],\n","        [235, 206, 135],\n","        [255, 255, 255]] \n","\n","    def one_hot_encode_mask(mask, unique_colors):\n","        one_hot_mask = np.zeros((*mask.shape[:2], len(unique_colors)), dtype=np.uint8)\n","        for idx, color in enumerate(unique_colors):\n","            class_mask = np.all(mask == color, axis=-1)\n","            one_hot_mask[..., idx] = class_mask.astype(np.uint8)  \n","        return one_hot_mask\n","\n","    # Iterate through mask images in the directory\n","    for filename in os.listdir(mask_directory):\n","        if filename.endswith('.png'):  # Assuming mask images are in PNG format\n","            # Load the mask image\n","            mask_image_path = os.path.join(mask_directory, filename)\n","            mask_image = cv2.imread(mask_image_path, cv2.IMREAD_UNCHANGED)\n","            mask_image = np.transpose(mask_image, (1, 0, 2))\n","            filtered_unique_colors = unique_colors[:]\n","            one_hot_encoded_mask = one_hot_encode_mask(mask_image, filtered_unique_colors)   \n","            output_filename = os.path.join(output_directory, filename.replace('.png', '.npy'))\n","            np.save(output_filename, one_hot_encoded_mask)"]},{"cell_type":"markdown","metadata":{},"source":["## Visulizing The Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T15:48:25.474822Z","iopub.status.busy":"2024-04-06T15:48:25.474536Z","iopub.status.idle":"2024-04-06T15:48:27.564525Z","shell.execute_reply":"2024-04-06T15:48:27.563330Z","shell.execute_reply.started":"2024-04-06T15:48:25.474799Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def visualize_batch(generator, num_images=5):\n","    # Generate a batch of data\n","    X, y = generator[0]  # Get the first batch from the generator\n","    \n","    # Select num_images random indices from the batch\n","    indices = np.random.choice(range(X.shape[0]), size=num_images, replace=False)\n","    \n","    # Plot the images and masks\n","    plt.figure(figsize=(15, 10))\n","    for i, idx in enumerate(indices):\n","        # Plot original image\n","        plt.subplot(num_images, 2, 2*i + 1)\n","        plt.imshow(X[idx])\n","        plt.title(f'Image {idx}')\n","        plt.axis('off')\n","\n","        # Plot mask (assuming mask is a one-hot encoded array)\n","        plt.subplot(num_images, 2, 2*i + 2)\n","        plt.imshow(np.argmax(y[idx], axis=-1), cmap='viridis')  # Displaying mask as an image\n","        plt.title(f'Mask {idx}')\n","        plt.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Visualize training images and masks\n","visualize_batch(train_generator, num_images=5)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Defining Model Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T15:48:27.566895Z","iopub.status.busy":"2024-04-06T15:48:27.566597Z","iopub.status.idle":"2024-04-06T15:48:27.578882Z","shell.execute_reply":"2024-04-06T15:48:27.577969Z","shell.execute_reply.started":"2024-04-06T15:48:27.566869Z"},"trusted":true},"outputs":[],"source":["def conv_block(input, num_filters):\n","    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","    \n","    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","\n","    return x\n","\n","def encoder_block(input, num_filters):\n","    x = conv_block(input, num_filters)\n","    p = MaxPool2D((2, 2))(x)\n","    return x, p\n","\n","def decoder_block(input, skip_features, num_filters):\n","    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n","    x = Concatenate()([x, skip_features])\n","    x = conv_block(x, num_filters)\n","    return x\n","\n","def build_unet(input_shape,num_classes):\n","    inputs = Input(input_shape)\n","\n","    s1, p1 = encoder_block(inputs, 32)\n","    s2, p2 = encoder_block(p1, 64)\n","    s3, p3 = encoder_block(p2, 128)\n","    s4, p4 = encoder_block(p3, 256)\n","\n","    b1 = conv_block(p4, 512)\n","\n","    d1 = decoder_block(b1, s4, 256)\n","    d2 = decoder_block(d1, s3, 128)\n","    d3 = decoder_block(d2, s2, 64)\n","    d4 = decoder_block(d3, s1, 32)\n","\n","    outputs = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(d4)\n","\n","    model = Model(inputs, outputs, name=\"U-Net\")\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T15:51:39.903296Z","iopub.status.busy":"2024-04-06T15:51:39.902903Z","iopub.status.idle":"2024-04-06T15:51:40.331007Z","shell.execute_reply":"2024-04-06T15:51:40.330087Z","shell.execute_reply.started":"2024-04-06T15:51:39.903269Z"},"trusted":true},"outputs":[],"source":["input_shape = (img_dim, img_dim, 3)\n","model = build_unet(input_shape, num_classes)\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","loss='categorical_crossentropy', # Use 'categorical_crossentropy' for multi-class\n","metrics=['accuracy']) # It's better to start with 'accuracy'\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## Creating function for Early Stopping"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T18:00:01.746740Z","iopub.status.busy":"2024-04-06T18:00:01.746051Z","iopub.status.idle":"2024-04-06T18:00:01.760936Z","shell.execute_reply":"2024-04-06T18:00:01.758650Z","shell.execute_reply.started":"2024-04-06T18:00:01.746708Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","def iou_coef(y_true, y_pred, smooth=1):\n","    y_true = tf.cast(y_true, tf.float32)\n","    y_pred = tf.cast(y_pred, tf.float32)\n","    \n","    intersection = tf.reduce_sum(tf.abs(y_true * y_pred), axis=[1, 2, 3])\n","    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3]) - intersection\n","    iou = tf.reduce_mean((intersection + smooth) / (union + smooth), axis=0)\n","    \n","    return iou\n","\n","\n","# Use IoU in your model's metrics\n","model.compile(optimizer='adam', \n","              loss='categorical_crossentropy', \n","              metrics=[iou_coef, 'accuracy'])\n","\n","checkpoint = ModelCheckpoint(\n","    'unet_multiclass_iou.keras', \n","    monitor='val_iou_coef',\n","    verbose=1, \n","    save_best_only=True, \n","    mode='max'\n",")\n","# Early stopping based on IoU\n","earlystop = EarlyStopping(monitor='val_iou_coef', \n","                          patience=10, \n","                          verbose=1, \n","                          mode='max',\n","                          restore_best_weights=True)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Loading Training and Validation data for Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T15:51:44.313194Z","iopub.status.busy":"2024-04-06T15:51:44.312302Z","iopub.status.idle":"2024-04-06T15:51:44.326209Z","shell.execute_reply":"2024-04-06T15:51:44.325363Z","shell.execute_reply.started":"2024-04-06T15:51:44.313165Z"},"trusted":true},"outputs":[],"source":["# Next, create instances of the NPYDataGenerator for training and validation data\n","train_data_generator = NPYDataGenerator(\n","    images_path='/kaggle/input/cv-road-marking/augmented_dataset/images/train',  # Replace with your train images .npy path\n","    masks_path='/kaggle/input/cv-road-marking/augmented_dataset/masks/train',    # Replace with your train masks .npy path\n","    batch_size=32,\n","    dim=(img_dim, img_dim),\n","    n_channels=3,\n","    n_classes=num_classes,  # Replace with your actual number of classes\n","    shuffle=True\n",")\n","\n","val_data_generator = NPYDataGenerator(\n","    images_path='/kaggle/input/cv-road-marking/augmented_dataset/images/valid',  # Replace with your validation images .npy path\n","    masks_path='/kaggle/input/cv-road-marking/augmented_dataset/masks/valid',    # Replace with your validation masks .npy path\n","    batch_size=32,\n","    dim=(img_dim, img_dim),\n","    n_channels=3,\n","    n_classes=num_classes,  # Replace with your actual number of classes\n","    shuffle=True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T15:51:46.430994Z","iopub.status.busy":"2024-04-06T15:51:46.430129Z","iopub.status.idle":"2024-04-06T16:20:59.998367Z","shell.execute_reply":"2024-04-06T16:20:59.997373Z","shell.execute_reply.started":"2024-04-06T15:51:46.430961Z"},"trusted":true},"outputs":[],"source":["history = model.fit(\n","    train_data_generator,\n","    validation_data=val_data_generator,\n","    epochs=50,\n","    callbacks=[checkpoint, earlystop]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T16:27:46.521200Z","iopub.status.busy":"2024-04-06T16:27:46.520630Z","iopub.status.idle":"2024-04-06T16:27:46.978664Z","shell.execute_reply":"2024-04-06T16:27:46.977749Z","shell.execute_reply.started":"2024-04-06T16:27:46.521173Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(20,8))\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model Loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.ylim([0,2])\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T16:29:39.412527Z","iopub.status.busy":"2024-04-06T16:29:39.412142Z","iopub.status.idle":"2024-04-06T16:29:51.562968Z","shell.execute_reply":"2024-04-06T16:29:51.562099Z","shell.execute_reply.started":"2024-04-06T16:29:39.412495Z"},"trusted":true},"outputs":[],"source":["# Initialize the data generator for your test data\n","test_images_path = '/kaggle/input/cv-road-marking/augmented_dataset/images/test'  # replace with your test images path\n","test_masks_path = '/kaggle/input/cv-road-marking/augmented_dataset/masks/test'  # replace with your test masks path\n","\n","test_generator = NPYDataGenerator(\n","    images_path=test_images_path,\n","    masks_path=test_masks_path,\n","    batch_size=5,  # Set batch size to 1 for demonstration purposes\n","    dim=(256, 256),\n","    n_channels=3,\n","    n_classes=5,  # Set this to your actual number of classes\n","    shuffle=False\n",")\n","\n","import tensorflow as tf\n","\n","# Load one batch of data\n","test_images, test_masks = test_generator.__getitem__(0)\n","\n","predicted_masks = model.predict(test_images)\n","\n","predicted_masks = np.argmax(predicted_masks, axis=-1)\n","\n","num_images_to_show = min(5, len(test_images))\n","\n","for i in range(num_images_to_show):\n","#     print(\"x\")\n","    plt.figure(figsize=(20, 8))\n","    # Display the original image\n","    plt.subplot(1, 3, 1)\n","    plt.imshow(test_images[i])\n","    plt.title('Original Image')\n","    plt.axis('off')\n","\n","    # Display the ground truth mask\n","    plt.subplot(1, 3, 2)\n","    ground_truth = np.argmax(test_masks[i], axis=-1)  # Assuming the masks are one-hot encoded\n","    plt.imshow(ground_truth, )\n","    plt.title('Ground Truth Mask')\n","    plt.axis('off')\n","\n","    # Display the predicted mask\n","    plt.subplot(1, 3, 3)\n","    plt.imshow(predicted_masks[i],)\n","    plt.title('Predicted Mask')\n","    plt.axis('off')\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Saving the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T16:30:39.466168Z","iopub.status.busy":"2024-04-06T16:30:39.465443Z","iopub.status.idle":"2024-04-06T16:30:39.939067Z","shell.execute_reply":"2024-04-06T16:30:39.937791Z","shell.execute_reply.started":"2024-04-06T16:30:39.466139Z"},"trusted":true},"outputs":[],"source":["model.save('unet_multiclass_iou_good.keras')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T17:58:56.742373Z","iopub.status.busy":"2024-04-06T17:58:56.741999Z","iopub.status.idle":"2024-04-06T17:58:57.316491Z","shell.execute_reply":"2024-04-06T17:58:57.314699Z","shell.execute_reply.started":"2024-04-06T17:58:56.742341Z"},"trusted":true},"outputs":[],"source":["# # This is to load the model which was saved\n","# import tensorflow as tf\n","\n","# # Load the saved model\n","# model = tf.keras.models.load_model('unet_multiclass_iou_good.keras')\n","\n","# # Use the loaded model for inference, evaluation, or further training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T18:00:16.041327Z","iopub.status.busy":"2024-04-06T18:00:16.040723Z","iopub.status.idle":"2024-04-06T18:00:17.450569Z","shell.execute_reply":"2024-04-06T18:00:17.449614Z","shell.execute_reply.started":"2024-04-06T18:00:16.041285Z"},"trusted":true},"outputs":[],"source":["# print(test_images.shape)\n","# predicted_masks = model.predict(test_images)\n","# print(predicted_masks.shape)\n","# predicted_masks[0].shape\n"]},{"cell_type":"markdown","metadata":{},"source":["## Plotting Image and Predicted Mask"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T19:18:48.029996Z","iopub.status.busy":"2024-04-06T19:18:48.029106Z","iopub.status.idle":"2024-04-06T19:18:48.644186Z","shell.execute_reply":"2024-04-06T19:18:48.643365Z","shell.execute_reply.started":"2024-04-06T19:18:48.029963Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def mask_to_color_image(predicted_mask, class_colors):\n","    color_mask = np.zeros((predicted_mask.shape[0], predicted_mask.shape[1], 3), dtype=np.uint8)\n","\n","    for class_label, color in enumerate(class_colors):\n","        color_mask[predicted_mask == class_label] = color\n","\n","    return color_mask\n","\n","def resize_image(image, target_size):\n","    resized_image = cv2.resize(image, target_size[::-1], interpolation=cv2.INTER_NEAREST)\n","    return resized_image\n","\n","# Define the class colors corresponding to your model's output classes\n","class_colors = [\n","    (0, 0, 0),# black Background\n","    (255, 0, 0), # red traffic sign\n","    (0, 255, 0), # green Divider\n","    (0, 0, 255), # blue lane marking\n","    (255, 255, 255),# white Zebra- crossing\n","]\n","\n","# Path to your original image\n","image_path = '/kaggle/input/cv-road-marking/augmented_dataset/images/test/v_1_64.jpg'  # Update with your image path\n","\n","# Load and resize the original image to 256x256\n","original_image = cv2.imread(image_path)\n","resized_image = cv2.resize(original_image, (256, 256))\n","\n","\n","\n","def resize_image_pillow(image_path, target_size=(256, 256), quality=100):\n","    # Open the image using Pillow\n","    image = Image.open(image_path)\n","\n","    # Resize the image while preserving aspect ratio\n","    image.thumbnail(target_size, Image.ANTIALIAS)\n","\n","    # Convert the image to RGB (if not already in RGB mode)\n","    image = image.convert('RGB')\n","\n","    # Save the resized image to a BytesIO object to control JPEG quality\n","    output = io.BytesIO()\n","    image.save(output, format='JPEG', quality=quality)\n","\n","    # Read the resized image back from the BytesIO object\n","    resized_image = Image.open(output)\n","    resized_image = np.array(resized_image)  # Convert PIL Image to NumPy array\n","\n","    return resized_image\n","\n","\n","image_size = (original_image.shape[0], original_image.shape[1])\n","\n","# Convert the resized image to a numpy array and normalize it\n","input_image = resized_image.astype(np.float32) / 255.0\n","\n","# Reshape the input image to match model's input shape (e.g., batch size of 1)\n","input_image = np.expand_dims(input_image, axis=0)\n","\n","# Use the model to predict masks for the input image\n","predicted_masks = model.predict(input_image)\n","\n","# Assuming predicted_masks[0] is the predicted mask for the first image in the batch\n","predicted_labels = np.argmax(predicted_masks[0], axis=-1)\n","\n","# Create the color mask from predicted labels\n","color_mask = mask_to_color_image(predicted_labels, class_colors)\n","\n","# Resize the color mask to the desired size (e.g., 960x576)\n","target_size = image_size\n","resized_mask = resize_image(color_mask, target_size)\n","\n","# Plot the original image and resized color mask side by side\n","plt.figure(figsize=(16, 8))\n","plt.subplot(2, 1, 1)\n","plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n","plt.title('Original Image')\n","plt.axis('off')\n","\n","plt.subplot(2, 1, 2)\n","plt.imshow(resized_mask)\n","plt.title('Road Marking Mask (Multiclass)')\n","plt.axis('off')\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Detection and classification of mask on original image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-06T19:19:05.468360Z","iopub.status.busy":"2024-04-06T19:19:05.467476Z","iopub.status.idle":"2024-04-06T19:19:06.091631Z","shell.execute_reply":"2024-04-06T19:19:06.090736Z","shell.execute_reply.started":"2024-04-06T19:19:05.468326Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# If possible load the saved Model Here\n","\n","def find_pixel_length_width_color(mask_image, original_image):\n","    \n","    gray = cv2.cvtColor(mask_image, cv2.COLOR_BGR2GRAY)\n","\n","    contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    dimensions = []\n","    contour_colors = []\n","    contour_labels = []\n","\n","    reference_colors = {\n","        \"Lane marking\": (0, 0, 255),\n","        \"Lane Edge\": (0, 255, 0),\n","        \"Zebra Crossing\": (255, 255, 255),\n","        \"Traffic sign\": (255, 0, 0)\n","    }\n","\n","    for contour in contours:\n","        # Minimum area rectangle\n","        area = cv2.contourArea(contour)\n","\n","        # Check if contour area is above the minimum threshold\n","        # (if area of contour is less than 150 don't consider it)\n","        if area < 150:\n","            continue \n","        \n","        rect = cv2.minAreaRect(contour)\n","        box = cv2.boxPoints(rect)  \n","        box = np.int0(box)  \n","\n","        # Calculate dimensions\n","        width = np.linalg.norm(box[0] - box[1])\n","        height = np.linalg.norm(box[1] - box[2])\n","        dimensions.append((width, height))\n","\n","        \n","        mask = np.zeros(gray.shape, np.uint8)\n","        cv2.drawContours(mask, [contour], -1, 255, -1)\n","        mean_color = cv2.mean(mask_image, mask=mask)[:3]\n","        contour_colors.append(mean_color)\n","\n","        \n","        min_distance = float('inf')\n","        assigned_label = \"Unknown\" # This is for marking which doesn't get identified\n","\n","        for label, color in reference_colors.items():\n","            distance = np.linalg.norm(np.array(mean_color) - np.array(color))\n","            if distance < min_distance:\n","                min_distance = distance\n","                assigned_label = label\n","\n","        contour_labels.append(assigned_label)\n","\n","        cv2.drawContours(original_image, [box], 0, (0, 0, 255), 2)\n","\n","        # Get bounding box coordinates\n","        x, y, w, h = cv2.boundingRect(contour)\n","\n","        # Place text label inside the bounding box\n","        cv2.putText(original_image, f\"{len(contour_labels)}) {assigned_label}\", (x + int(w * 0.05), y + int(h * 0.5)),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n","\n","    # Plot the image with bounding boxes and labels\n","    plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n","    plt.title('Marking Detection and Classification')\n","    plt.axis('off')\n","    plt.show()\n","\n","    return dimensions, contour_colors, contour_labels\n","\n","original_image = cv2.imread(image_path)\n","# Operations on original Image (to make in compatible to model input)\n","image_size = (original_image.shape[0], original_image.shape[1])\n","input_image = resized_image.astype(np.float32) / 255.0\n","input_image = np.expand_dims(input_image, axis=0)\n","\n","# Model Predicts\n","predicted_masks = model.predict(input_image) \n","\n","# Operation on predicted mask for proper visualization\n","predicted_labels = np.argmax(predicted_masks[0], axis=-1)\n","color_mask = mask_to_color_image(predicted_labels, class_colors)\n","target_size = image_size\n","resized_mask = resize_image(color_mask, target_size)\n","mask_image = resized_mask\n","\n","dimensions, contour_colors, contour_labels = find_pixel_length_width_color(mask_image, original_image)\n","\n","# This will display details about each markings\n","for i, ((width, height), color, label) in enumerate(zip(dimensions, contour_colors, contour_labels), start=1):\n","    print(f\"Mark {i}:'{label}'  Width = {width:.2f}px, Height = {height:.2f}px,\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4742674,"sourceId":8043711,"sourceType":"datasetVersion"},{"datasetId":4743400,"sourceId":8044667,"sourceType":"datasetVersion"},{"datasetId":4743739,"sourceId":8045140,"sourceType":"datasetVersion"},{"datasetId":4745458,"sourceId":8047535,"sourceType":"datasetVersion"},{"datasetId":4745742,"sourceId":8047942,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
